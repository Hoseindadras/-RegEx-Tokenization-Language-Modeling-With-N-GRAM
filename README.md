# Natural Language Processing (NLP) Project

This repository explores key NLP techniques, including Regular Expressions (RegEx), Tokenization, Named Entity Recognition (NER), Language Modeling using N-Gram models, and an extension into Image Tokenization for Vision Transformers. This project serves as an end-to-end journey through foundational NLP tasks, language model training, and classifier analysis.

## Author
- **Name**: Hosein Dadras
- **Personal Mail**: [hoseindadras6@gmail.com](mailto:hoseindadras6@gmail.com)
- **Academic Mail**: [hoseindadras@ut.ac.ir](mailto:hoseindadras@ut.ac.ir)

## Table of Contents

1. **Chapter 1: RegEx, NER, Tokenization & Edit Distances (ED)**
    - **CH1-Data**: Overview of the `ner.txt` dataset
    - **Part 1.1**: Named Entity Recognition (NER) & Using RegEx in NER
    - **Part 1.2**: Regular Expressions (RegEx) Applications
    - **Part 1.3**: Advantages & Disadvantages of Using RegEx in NER
    - **Part 2.1**: Rule-Based vs. ML-Based Tokenizations
    - **Part 2.2**: Whitespace Tokenization Techniques
    - **Part 3.1**: Levenshtein & Damerau-Levenshtein Distances
    - **Part 3.2**: Implementation of Edit Distances

2. **Chapter 2: N-Gram Language Models**
    - **CH2-Data**: Dataset Information
    - **Part 1.1**: WordPiece Methodology
    - **Part 1.2**: Models Using WordPiece
    - **Part 1.3**: Comparison with Byte-Pair Encoding (BPE)
    - **Part 2.1**: Training a WordPiece Tokenizer on Ferdowsi's Text
    - **Part 2.2**: Testing & Tokenization with Trained Tokenizer
    - **Part 3.1**: N-Gram Language Model with Previous Tokenizer
    - **Part 3.2**: Model Training & Text Generation
    - **Part 3.3**: Analysis of Different "N" Sizes in N-Gram
    - **Part 3.A**: Generated Text Analysis
    - **Part 4.1**: Perplexity Criterion
    - **Part 4.2**: Perplexity Analysis on Hafez & Modern Persian Poetry
    - **Part 4.A**: Perplexity Analysis Results

3. **Chapter 3: N-Gram as a Classifier**
    - **Part 1.1**: N-Gram as a Classifier
    - **Part 1.2**: Challenges with Large "N" and Limited Data
    - **Part 2.1**: BPE Tokenizer Implementation
    - **Part 2.2**: Dataset Tokenization
    - **Part 3.1**: N-Gram-based Classification Techniques
    - **Part 3.2**: Analysis of 2- and 3-Gram Models
    - **Part 3.3**: Model Evaluation Criteria
    - **Part 3.4**: Results Analysis

4. **Chapter 4: Image Tokenization**
    - **Part 1.1**: Image Tokenization & Vision Transformers
    - **Part 1.2**: Tokenizing MNIST Dataset with KMeans
    - **Part 2.1**: Analysis with 4-Gram Model
    - **Part 2.2**: Model Accuracy
    - **Part 2.plot**: Visualization of Predictions
